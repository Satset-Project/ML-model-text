# -*- coding: utf-8 -*-
"""Testing model by Alfa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MlFhxSq8D4VV93eHcrn38nAXvKtpiTgd
"""

pip install tensorflow pandas scikit-learn

import pandas as pd

# Load the dataset with the correct delimiter
file_path = '/content/sample_data/dummy_dataset_alfa.csv'

try:
    df = pd.read_csv(file_path, delimiter=';')
    print(df.head())
except pd.errors.ParserError as e:
    print("Error parsing file:", e)

"""Preprocess Data"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Parameters
max_words = 5000
max_len = 50

# Tokenization and padding
tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')
tokenizer.fit_on_texts(df['Description'].values)
X = tokenizer.texts_to_sequences(df['Description'].values)
X = pad_sequences(X, maxlen=max_len)

# Encode labels
category_labels = df['Category'].unique().tolist()
service_labels = df['Service'].unique().tolist()

df['Category'] = df['Category'].apply(lambda x: category_labels.index(x))
df['Service'] = df['Service'].apply(lambda x: service_labels.index(x))

y_category = to_categorical(df['Category'].values)
y_service = to_categorical(df['Service'].values)

# Split the data
X_train, X_test, y_category_train, y_category_test, y_service_train, y_service_test = train_test_split(
    X, y_category, y_service, test_size=0.2, random_state=42)

"""Build and train model"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM, Dense

# Model parameters
embedding_dim = 128

# Input layer
input = Input(shape=(max_len,))
embedding = Embedding(max_words, embedding_dim, input_length=max_len)(input)
dropout = SpatialDropout1D(0.2)(embedding)
lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(dropout)
dense_shared = Dense(128, activation='relu')(lstm)

# Output layers for multi-task learning
output_category = Dense(len(category_labels), activation='softmax', name='category_output')(dense_shared)
output_service = Dense(len(service_labels), activation='softmax', name='service_output')(dense_shared)

model = Model(inputs=input, outputs=[output_category, output_service])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())

# Train the model
history = model.fit(
    X_train,
    [y_category_train, y_service_train],
    epochs=5,
    batch_size=64,
    validation_data=(X_test, [y_category_test, y_service_test])
)

"""Evaluate Model"""

# Evaluate the model
loss, category_loss, service_loss, category_accuracy, service_accuracy = model.evaluate(
    X_test, [y_category_test, y_service_test], verbose=1)

print(f'Category Accuracy: {category_accuracy}')
print(f'Service Accuracy: {service_accuracy}')

"""Save model"""

model.save('satset_text_classification_model.h5')

